{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, glob\n",
    "import librosa, mir_eval, guitarpro\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGLE_TRACK_GTP_DIR = \"/Volumes/MacOnly/UG_rewrite/all_time_top_by_hits/clean_single_track_gtps\"\n",
    "SINGLE_TRACK_ANNO_DIR = \"/Volumes/MacOnly/UG_rewrite/all_time_top_by_hits/clean_single_track_annos\"\n",
    "SINGLE_TRACK_AUDIO_DIR = \"/Volumes/MacOnly/UG_rewrite/all_time_top_by_hits/clean_single_track_audio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_vs_mono_vs_silence(song):\n",
    "    \"\"\"Return the time stamps for the start and end of each monophonic / polyphonic / silence segments in the song\n",
    "\n",
    "    Args:\n",
    "        song (Song): A pyguitarpro Song object. The song to analyze\n",
    "\n",
    "    Returns:\n",
    "        list, list, list: A list of (start, end) time stamps for all mono segments, poly segments, and silence segments\n",
    "    \"\"\"\n",
    "    bpm = song.tempo\n",
    "    poly_segments = []\n",
    "    mono_segments = []\n",
    "    silence_segments = []\n",
    "\n",
    "    previous_beat_status = -1\n",
    "    beats = []\n",
    "    for measure in song.tracks[0].measures:\n",
    "        voice = measure.voices[0]\n",
    "        beats.extend(voice.beats)\n",
    "    for beat in beats:\n",
    "        onset = beat.start\n",
    "        onset_sec = round(((onset - 960) / 960) / (bpm / 60), 4)\n",
    "        dur = beat.duration.time\n",
    "        dur_sec = round((dur / 960) / (bpm / 60), 4)\n",
    "        offset_sec = onset_sec + dur_sec\n",
    "        # 2 for polyphonic, 1 for monophonic, 0 for silence\n",
    "        if len(beat.notes) == 0:\n",
    "            beat_status = 0\n",
    "        elif len(beat.notes) == 1:\n",
    "            beat_status = 1\n",
    "        else:\n",
    "            beat_status = 2\n",
    "        if beat_status != previous_beat_status:\n",
    "            # if current beat status is different from the previous beat, add the timing to the output list\n",
    "            # the following lines can obviously be better written, I leave it like this just for clarity\n",
    "            if beat_status == 2:\n",
    "                poly_segments.append([onset_sec, offset_sec])\n",
    "            elif beat_status == 1:\n",
    "                mono_segments.append([onset_sec, offset_sec])\n",
    "            else:\n",
    "                assert beat_status == 0\n",
    "                silence_segments.append([onset_sec, offset_sec])\n",
    "        else:\n",
    "            # if current beat status is the same as the previous one, update the offset of the previous entry\n",
    "            if beat_status == 2:\n",
    "                poly_segments[-1][1] = offset_sec\n",
    "            elif beat_status == 1:\n",
    "                mono_segments[-1][1] = offset_sec\n",
    "            else:\n",
    "                assert beat_status == 0\n",
    "                silence_segments[-1][1] = offset_sec\n",
    "        previous_beat_status = beat_status\n",
    "    return poly_segments, mono_segments, silence_segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f0_note_tracker(y, sr, frame_size, hop_size, pitch_diff_thres, note_dur_thres):\n",
    "    \"\"\"Note tracking based on F0 segmentation\n",
    "\n",
    "    This function takes an audio signal as input and use PYIN to estimate an f0 curve.\n",
    "    The curve is cut at points where the pitch difference between two adjacent frames exceeds `pitch_diff_thres`.\n",
    "    Spurious notes shorter than `note_dur_thres` are discarded.\n",
    "\n",
    "    Args:\n",
    "        y (array): audio signal\n",
    "        pitch_diff_thres (float, optional): F0 cutting point threshold.\n",
    "        note_dur_thres (int, optional): Estimated notes whose duration is shorter than this threshold is discarded.\n",
    "\n",
    "    Returns:\n",
    "        array: Estimated intervals: [(onset, offset)]\n",
    "    \"\"\"\n",
    "    est_intervals = []\n",
    "\n",
    "    # if the segment is shorter than note_dur_thres, just discard it\n",
    "    if len(y) < note_dur_thres/1000*sr:\n",
    "        return []\n",
    "        \n",
    "    f0, _, _ = librosa.pyin(\n",
    "        y,\n",
    "        fmin=librosa.note_to_hz(\"C2\"),\n",
    "        fmax=librosa.note_to_hz(\"G6\"),\n",
    "        sr=sr,\n",
    "        frame_length=frame_size,\n",
    "        hop_length=hop_size,\n",
    "        center=True,\n",
    "    )\n",
    "    times = librosa.times_like(f0, sr=sr, hop_length=hop_size)\n",
    "    notes = librosa.hz_to_midi(f0)\n",
    "\n",
    "    note_events = np.split(notes, np.where(abs(np.diff(notes)) > pitch_diff_thres)[0] + 1)\n",
    "    time_intervals = np.split(times, np.where(abs(np.diff(notes)) > pitch_diff_thres)[0] + 1)\n",
    "\n",
    "    for i in range(len(note_events)):\n",
    "        time_interval = time_intervals[i]\n",
    "        # ignore spurious note events\n",
    "        if len(time_interval) > librosa.time_to_frames(note_dur_thres/1000, sr=sr, hop_length=hop_size):\n",
    "            est_intervals.append([time_interval[0], time_interval[-1]])\n",
    "    \n",
    "    est_intervals = np.array(est_intervals)\n",
    "\n",
    "    return est_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# considering the time resolution, a frame size of 8192 is 186ms, the frame is way too long\n",
    "# FRAME_SIZE = [2048, 4096]\n",
    "FRAME_SIZE = [1024]\n",
    "HOP_SIZE_RATIO = [0.25, 0.5, 0.75]\n",
    "PDT = [0.1, 0.2, 0.4, 0.8]\n",
    "NDT = [25, 50, 75]\n",
    "\n",
    "for frame_size in FRAME_SIZE:\n",
    "    for hop_size_ratio in HOP_SIZE_RATIO:\n",
    "        hop_size = int(frame_size * hop_size_ratio)\n",
    "        for pdt in PDT:\n",
    "            for ndt in NDT:\n",
    "                print(f\"FR: {frame_size}, HOP: {hop_size}, PDT: {pdt}, NDT: {ndt}\")\n",
    "                # bypass the mono detector, test onset note tracker on strictly mono segments\n",
    "                matching_cnt = 0\n",
    "                est_cnt = 0\n",
    "                anno_cnt = 0\n",
    "\n",
    "                for audio_file in glob.glob(os.path.join(SINGLE_TRACK_AUDIO_DIR, \"*.wav\"))[:20]:\n",
    "                    track_name, _ = os.path.splitext(audio_file.split(\"/\")[-1])\n",
    "                    gtp_file = os.path.join(SINGLE_TRACK_GTP_DIR, track_name + \".gp5\")\n",
    "                    anno_file = os.path.join(SINGLE_TRACK_ANNO_DIR, track_name + \".json\")\n",
    "\n",
    "                    all_est_intervals = []\n",
    "\n",
    "                    y, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "                    p, m, s = poly_vs_mono_vs_silence(guitarpro.parse(gtp_file))\n",
    "                    for mono_timestamp in m:\n",
    "                        start_sp = int(mono_timestamp[0] * sr)\n",
    "                        end_sp = int(mono_timestamp[1] * sr)\n",
    "                        mono_segment = y[start_sp : end_sp]\n",
    "                        est_intervals = f0_note_tracker(mono_segment, sr, frame_size=frame_size, hop_size=hop_size, pitch_diff_thres=pdt, note_dur_thres=ndt)\n",
    "                        if len(est_intervals)==0:\n",
    "                            continue\n",
    "                        # convert to global time\n",
    "                        est_intervals = est_intervals + mono_timestamp[0]\n",
    "                        all_est_intervals.append(est_intervals)\n",
    "\n",
    "                    if not all_est_intervals:\n",
    "                        continue\n",
    "\n",
    "                    all_est_intervals = np.concatenate(all_est_intervals, axis=0)\n",
    "\n",
    "                    all_ref_intervals = []\n",
    "                    with open(anno_file) as anno:\n",
    "                        annotation = json.load(anno)\n",
    "                    for note_event in annotation:\n",
    "                        if note_event[\"time\"][\"dur\"] > frame_size / sr:\n",
    "                            onset_time = note_event[\"time\"][\"start\"]\n",
    "                            offset_time = note_event[\"time\"][\"start\"] + note_event[\"time\"][\"dur\"]\n",
    "                            all_ref_intervals.append([onset_time, offset_time])\n",
    "                    all_ref_intervals = np.array(all_ref_intervals)\n",
    "\n",
    "                    onset_matching = mir_eval.transcription.match_note_onsets(all_ref_intervals, all_est_intervals)\n",
    "                    offset_matching = mir_eval.transcription.match_note_offsets(all_ref_intervals, all_est_intervals)\n",
    "                    matching = [match for match in onset_matching if match in offset_matching]\n",
    "\n",
    "                    matching_cnt += len(matching)\n",
    "                    est_cnt += len(all_est_intervals)\n",
    "                    anno_cnt += len(all_ref_intervals)\n",
    "\n",
    "                precision = matching_cnt/est_cnt\n",
    "                recall = matching_cnt/anno_cnt\n",
    "                f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "                print(matching_cnt, est_cnt, anno_cnt)\n",
    "                print(f\"precision: {precision}, recall: {recall}, f1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onset detection-based note-event separation heuristics\n",
    "def onset_note_tracker(y, sr, frame_size, hop_size, note_dur_thres, backtrack=False):\n",
    "    est_intervals = []\n",
    "\n",
    "    # S = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=frame_size, hop_length=hop_size, n_mels=128, fmax=8000)\n",
    "    # S = librosa.power_to_db(S)\n",
    "    # onset_envelop = librosa.onset.onset_strength(y=None, sr=sr, S=S)\n",
    "\n",
    "    onset_envelop = librosa.onset.onset_strength(y=y, sr=sr, n_fft=frame_size, hop_length=hop_size)\n",
    "\n",
    "    onsets = librosa.onset.onset_detect(y=None, sr=sr, onset_envelope=onset_envelop, hop_length=hop_size, units='time', backtrack=backtrack)\n",
    "\n",
    "    # the onset detection tend to ignore the first note in the audio\n",
    "    # so add the time 0 to the onsets\n",
    "    onsets = np.insert(onsets, 0, 0)\n",
    "\n",
    "    for i in range(len(onsets)-1):\n",
    "        onset = onsets[i]\n",
    "        next_onset = onsets[i+1]\n",
    "        est_intervals.append([onset, next_onset])\n",
    "\n",
    "    # the last onset to the end of the audio\n",
    "    est_intervals.append([onsets[-1], len(y)/sr])\n",
    "\n",
    "    est_intervals = [interval for interval in est_intervals if interval[1] - interval[0] > note_dur_thres/1000]\n",
    "    est_intervals = np.array(est_intervals)\n",
    "    return est_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# considering the time resolution, a frame size of 8192 is 186ms, the frame is way too long\n",
    "FRAME_SIZE = [1024, 2048, 4096]\n",
    "HOP_SIZE_RATIO = [0.25, 0.5, 0.75]\n",
    "NDT = [25, 50, 75]\n",
    "\n",
    "for frame_size in FRAME_SIZE:\n",
    "    for hop_size_ratio in HOP_SIZE_RATIO:\n",
    "        hop_size = int(frame_size * hop_size_ratio)\n",
    "        for ndt in NDT:\n",
    "            print(f\"FR: {frame_size}, HOP: {hop_size}, NDT: {ndt}\")\n",
    "            # bypass the mono detector, test onset note tracker on strictly mono segments\n",
    "            matching_cnt = 0\n",
    "            est_cnt = 0\n",
    "            anno_cnt = 0\n",
    "\n",
    "            for audio_file in glob.glob(os.path.join(SINGLE_TRACK_AUDIO_DIR, \"*.wav\"))[:40]:\n",
    "                track_name, _ = os.path.splitext(audio_file.split(\"/\")[-1])\n",
    "                gtp_file = os.path.join(SINGLE_TRACK_GTP_DIR, track_name + \".gp5\")\n",
    "                anno_file = os.path.join(SINGLE_TRACK_ANNO_DIR, track_name + \".json\")\n",
    "\n",
    "                all_est_intervals = []\n",
    "\n",
    "                y, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "                p, m, s = poly_vs_mono_vs_silence(guitarpro.parse(gtp_file))\n",
    "                for mono_timestamp in m:\n",
    "                    start_sp = int(mono_timestamp[0] * sr)\n",
    "                    end_sp = int(mono_timestamp[1] * sr)\n",
    "                    mono_segment = y[start_sp : end_sp]\n",
    "                    est_intervals = onset_note_tracker(mono_segment, sr, frame_size=frame_size, hop_size=hop_size, note_dur_thres=ndt, backtrack=True)\n",
    "                    if len(est_intervals)==0:\n",
    "                        continue\n",
    "                    # convert to global time\n",
    "                    est_intervals = est_intervals + mono_timestamp[0]\n",
    "                    all_est_intervals.append(est_intervals)\n",
    "\n",
    "                if not all_est_intervals:\n",
    "                    continue\n",
    "\n",
    "                all_est_intervals = np.concatenate(all_est_intervals, axis=0)\n",
    "\n",
    "                all_ref_intervals = []\n",
    "                with open(anno_file) as anno:\n",
    "                    annotation = json.load(anno)\n",
    "                for note_event in annotation:\n",
    "                    if note_event[\"time\"][\"dur\"] > frame_size / sr:\n",
    "                        onset_time = note_event[\"time\"][\"start\"]\n",
    "                        offset_time = note_event[\"time\"][\"start\"] + note_event[\"time\"][\"dur\"]\n",
    "                        all_ref_intervals.append([onset_time, offset_time])\n",
    "                all_ref_intervals = np.array(all_ref_intervals)\n",
    "\n",
    "                onset_matching = mir_eval.transcription.match_note_onsets(all_ref_intervals, all_est_intervals)\n",
    "                offset_matching = mir_eval.transcription.match_note_offsets(all_ref_intervals, all_est_intervals)\n",
    "                matching = [match for match in onset_matching if match in offset_matching]\n",
    "\n",
    "                matching_cnt += len(matching)\n",
    "                est_cnt += len(all_est_intervals)\n",
    "                anno_cnt += len(all_ref_intervals)\n",
    "\n",
    "            precision = matching_cnt/est_cnt\n",
    "            recall = matching_cnt/anno_cnt\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "            print(matching_cnt, est_cnt, anno_cnt)\n",
    "            print(f\"precision: {precision}, recall: {recall}, f1: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e4299a6bfeee03721df81a04beb561ffa0442fcfce618d29889931a6d6e4527"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
