{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, glob\n",
    "import librosa, mir_eval, guitarpro, scipy\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGLE_TRACK_GTP_DIR = \"/Volumes/MacOnly/UG_rewrite/all_time_top_by_rating/clean_single_track_gtps\"\n",
    "SINGLE_TRACK_ANNO_DIR = \"/Volumes/MacOnly/UG_rewrite/all_time_top_by_rating/clean_single_track_annos\"\n",
    "SINGLE_TRACK_AUDIO_DIR = \"/Volumes/MacOnly/UG_rewrite/all_time_top_by_rating/clean_single_track_audio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(y, sr):\n",
    "    mfcc = librosa.feature.mfcc(y, sr=sr)\n",
    "    pitch, _, _ = librosa.pyin(y, fmin=librosa.note_to_hz(\"C2\"), fmax=librosa.note_to_hz(\"G6\"), sr=sr, fill_na=None)\n",
    "    centroid = np.squeeze(librosa.feature.spectral_centroid(y, sr))\n",
    "    bandwidth = np.squeeze(librosa.feature.spectral_bandwidth(y, sr))\n",
    "    flatness = np.squeeze(librosa.feature.spectral_flatness(y))\n",
    "    rolloff = np.squeeze(librosa.feature.spectral_rolloff(y, sr))\n",
    "    zero_crossing = np.squeeze(librosa.feature.zero_crossing_rate(y))\n",
    "    flux = librosa.onset.onset_strength(y, sr)\n",
    "\n",
    "    non_mfccs = np.array([pitch, centroid, bandwidth, flatness, rolloff, zero_crossing, flux])\n",
    "    features = np.concatenate((mfcc, non_mfccs), axis=0)\n",
    "\n",
    "    features_delta = librosa.feature.delta(features, order=1)\n",
    "    features_accel = librosa.feature.delta(features, order=2)\n",
    "\n",
    "    all_features = np.concatenate((features, features_delta, features_accel), axis=0) # (81, 13127)\n",
    "    assert all_features.shape[0] == 81\n",
    "    return all_features\n",
    "\n",
    "def get_all_stats(a):\n",
    "    \"\"\"Given a 2D matrix, compute and concatenate the 6 statistics.\n",
    "\n",
    "    Args:\n",
    "        a (array): The input time series, of the shape (n,)\n",
    "\n",
    "    Returns:\n",
    "        array: An array containing the statistics, of the shape (6,)\n",
    "    \"\"\"\n",
    "    assert a.ndim == 2\n",
    "    mean = np.mean(a, axis=1)\n",
    "    std = np.std(a, axis=1)\n",
    "    max = np.max(a, axis=1)\n",
    "    min = np.min(a, axis=1)\n",
    "    skewness = scipy.stats.skew(a, axis=1, nan_policy=\"raise\")\n",
    "    kurtosis = scipy.stats.kurtosis(a, axis=1, nan_policy=\"raise\")\n",
    "\n",
    "    stats = np.concatenate((mean, std, max, min, skewness, kurtosis), axis=0)\n",
    "    assert stats.shape[0] == a.shape[0] * 6\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f0_note_tracker(y, sr, pitch_diff_thres=0.8, note_dur_thres=75):\n",
    "    \"\"\"Note tracking based on F0 segmentation\n",
    "\n",
    "    This function takes an audio signal as input and use PYIN to estimate an f0 curve.\n",
    "    The curve is cut at points where the pitch difference between two adjacent frames exceeds `pitch_diff_thres`.\n",
    "    Spurious notes shorter than `note_dur_thres` are discarded.\n",
    "\n",
    "    Args:\n",
    "        y (array): audio signal\n",
    "        pitch_diff_thres (float, optional): F0 cutting point threshold. Defaults to 0.8.\n",
    "        note_dur_thres (int, optional): Estimated notes whose duration is shorter than this threshold is discarded. Defaults to 75ms.\n",
    "\n",
    "    Returns:\n",
    "        array: Estimated intervals: [(onset, offset)]\n",
    "    \"\"\"\n",
    "    est_intervals = []\n",
    "\n",
    "    # if the segment is shorter than note_dur_thres, just discard it\n",
    "    if len(y) < note_dur_thres/1000*sr:\n",
    "        return []\n",
    "        \n",
    "    f0, _, _ = librosa.pyin(\n",
    "        y,\n",
    "        fmin=librosa.note_to_hz(\"C2\"),\n",
    "        fmax=librosa.note_to_hz(\"G6\"),\n",
    "        sr=sr,\n",
    "        center=True,\n",
    "    )\n",
    "    times = librosa.times_like(f0, sr=sr)\n",
    "    notes = librosa.hz_to_midi(f0)\n",
    "\n",
    "    note_events = np.split(notes, np.where(abs(np.diff(notes)) > pitch_diff_thres)[0] + 1)\n",
    "    time_intervals = np.split(times, np.where(abs(np.diff(notes)) > pitch_diff_thres)[0] + 1)\n",
    "\n",
    "    for i in range(len(note_events)):\n",
    "        note_event = note_events[i]\n",
    "        time_interval = time_intervals[i]\n",
    "        # ignore spurious note events\n",
    "        if len(time_interval) > librosa.time_to_frames(note_dur_thres/1000, sr=sr):\n",
    "            est_intervals.append([time_interval[0], time_interval[-1]])\n",
    "    \n",
    "    est_intervals = np.array(est_intervals)\n",
    "    return est_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onset detection-based note-event separation heuristics\n",
    "def onset_note_tracker(y, sr, note_dur_thres=70, backtrack=False):\n",
    "    est_intervals = []\n",
    "    onsets = librosa.onset.onset_detect(y=y, sr=sr, units='time', backtrack=backtrack)\n",
    "\n",
    "    # the onset detection tend to ignore the first note in the audio\n",
    "    # so add the time 0 to the onsets\n",
    "    onsets = np.insert(onsets, 0, 0)\n",
    "\n",
    "    for i in range(len(onsets)-1):\n",
    "        onset = onsets[i]\n",
    "        next_onset = onsets[i+1]\n",
    "        est_intervals.append([onset, next_onset])\n",
    "\n",
    "    # the last onset to the end of the audio\n",
    "    est_intervals.append([onsets[-1], len(y)/sr])\n",
    "\n",
    "    est_intervals = [interval for interval in est_intervals if interval[1] - interval[0] > note_dur_thres/1000]\n",
    "    est_intervals = np.array(est_intervals)\n",
    "    return est_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_clf = joblib.load(\"/Users/jw/Documents/unified_clf.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_NOTE_DIR = \"/Users/jw/Documents/note_results\"\n",
    "RESULT_TRAN_DIR = \"/Users/jw/Documents/tran_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for audio_file in glob.glob(os.path.join(SINGLE_TRACK_AUDIO_DIR, \"*.wav\")):\n",
    "    track_name, _ = os.path.splitext(audio_file.split(\"/\")[-1])\n",
    "    print(track_name, i)\n",
    "    i += 1\n",
    "    gtp_file = os.path.join(SINGLE_TRACK_GTP_DIR, track_name + \".gp5\")\n",
    "    anno_file = os.path.join(SINGLE_TRACK_ANNO_DIR, track_name + \".json\")\n",
    "\n",
    "    all_est_notes = []\n",
    "    all_est_trans = []\n",
    "\n",
    "    y, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "    p, m, s = poly_vs_mono_vs_silence(guitarpro.parse(gtp_file))\n",
    "    for mono_timestamp in m:\n",
    "        start_sp = librosa.time_to_samples(mono_timestamp[0], sr)\n",
    "        end_sp = librosa.time_to_samples(mono_timestamp[1], sr)\n",
    "        mono_segment = y[start_sp : end_sp]\n",
    "        est_notes = onset_note_tracker(mono_segment, sr, note_dur_thres=50, backtrack=True)\n",
    "        # convert to global time\n",
    "        est_notes = est_notes + mono_timestamp[0]\n",
    "        if len(est_notes) != 0:\n",
    "            all_est_notes.append(est_notes)\n",
    "\n",
    "    all_note_features = []\n",
    "    all_tran_features = []\n",
    "\n",
    "    features = extract_features(y, sr)\n",
    "\n",
    "    for est_notes in all_est_notes:\n",
    "        for j in range(len(est_notes)):\n",
    "            est_note = est_notes[j]\n",
    "            onset_fr = librosa.time_to_frames(est_note[0], sr=sr)\n",
    "            offset_fr = librosa.time_to_frames(est_note[1], sr=sr)\n",
    "            # if note duration is shorter than one frame, discard it\n",
    "            if offset_fr - onset_fr < 1:\n",
    "                continue\n",
    "            note_feature = features[:, onset_fr : offset_fr+1]\n",
    "            note_aggregation = get_all_stats(note_feature)\n",
    "            all_note_features.append(note_aggregation)\n",
    "\n",
    "            if j != len(est_notes) - 1:\n",
    "                all_est_trans.append(est_note)\n",
    "                tran_feature = features[:, offset_fr-2 : offset_fr+3]\n",
    "                tran_aggregation = get_all_stats(tran_feature)\n",
    "                all_tran_features.append(tran_aggregation)\n",
    "\n",
    "    # if there is no estimated notes (when the whole track is poly)\n",
    "    if len(all_est_notes) == 0:\n",
    "        continue\n",
    "\n",
    "    all_est_notes = np.concatenate(all_est_notes, axis=0)\n",
    "    all_note_features = np.array(all_note_features)\n",
    "    all_tran_features = np.array(all_tran_features)\n",
    "\n",
    "    assert all_est_notes.shape[0] == all_note_features.shape[0]\n",
    "    assert all_tran_features.shape[0] <= all_note_features.shape[0]\n",
    "\n",
    "    y_pred_notes = pt_clf.predict(all_note_features)\n",
    "    y_pred_trans = pt_clf.predict(all_tran_features)\n",
    "\n",
    "    note_result = np.column_stack((all_est_notes, y_pred_notes))\n",
    "    tran_result = np.column_stack((all_est_trans, y_pred_trans))\n",
    "\n",
    "    result_note_file = os.path.join(RESULT_NOTE_DIR, track_name + \".csv\")\n",
    "    result_tran_file = os.path.join(RESULT_TRAN_DIR, track_name + \".csv\")\n",
    "\n",
    "    np.savetxt(result_note_file, note_result, fmt=[\"%.2f\", \"%.2f\", \"%d\"], delimiter=\",\")\n",
    "    np.savetxt(result_tran_file, tran_result, fmt=[\"%.2f\", \"%.2f\", \"%d\"], delimiter=\",\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_e2e(result_file):\n",
    "    note_result = np.loadtxt(os.path.join(RESULT_NOTE_DIR, result_file), delimiter=\",\")\n",
    "    tran_result = np.loadtxt(os.path.join(RESULT_TRAN_DIR, result_file), delimiter=\",\")\n",
    "\n",
    "    note_timestamps = note_result[:, :2]\n",
    "    note_labels = note_result[:, 2]\n",
    "\n",
    "    tran_timestamps = tran_result[:, :2]\n",
    "    tran_labels = tran_result[:, 2]\n",
    "\n",
    "    bend_timestamps = note_timestamps[note_labels==1]\n",
    "    vibrato_timestamps = note_timestamps[note_labels==2]\n",
    "    \n",
    "    hammer_timestamps = tran_timestamps[tran_labels==3]\n",
    "    pull_timestamps = tran_timestamps[tran_labels==4]\n",
    "    slide_timestamps = tran_timestamps[tran_labels==5]\n",
    "\n",
    "    track_name, _ = os.path.splitext(result_file.split(\"/\")[-1])\n",
    "    anno_file = os.path.join(SINGLE_TRACK_ANNO_DIR, track_name+\".json\")\n",
    "    with open(anno_file) as anno:\n",
    "        annotation = json.load(anno)\n",
    "\n",
    "    ref_bend_timestamps = []\n",
    "    ref_vibrato_timestamps = []\n",
    "    ref_hammer_timestamps = []\n",
    "    ref_pull_timestamps = []\n",
    "    ref_slide_timestamps = []\n",
    "\n",
    "    for i in range(len(annotation)):\n",
    "        note_event = annotation[i]\n",
    "\n",
    "        if note_event[\"time\"][\"dur\"] <= 2048 / sr:\n",
    "            continue\n",
    "\n",
    "        onset_time = note_event[\"time\"][\"start\"]\n",
    "        offset_time = note_event[\"time\"][\"start\"] + note_event[\"time\"][\"dur\"]\n",
    "\n",
    "        if note_event[\"effects\"][\"bend\"]:\n",
    "            ref_bend_timestamps.append([onset_time, offset_time])\n",
    "        if note_event[\"effects\"][\"vibrato\"]:\n",
    "            ref_vibrato_timestamps.append([onset_time, offset_time])\n",
    "\n",
    "        # if the note event is the last one in the song, ignore its transitions \n",
    "        if i == len(annotation) - 1:\n",
    "            break\n",
    "\n",
    "        next_note_event = annotation[i + 1]\n",
    "        # if the next note event doesn't immediately follow the current note event, ignore the transition\n",
    "        if next_note_event[\"time\"][\"start\"] - offset_time > 0.05:\n",
    "            continue\n",
    "\n",
    "        if note_event[\"effects\"][\"hammer\"]:\n",
    "            if note_event[\"pitch\"] < next_note_event[\"pitch\"]:\n",
    "                ref_hammer_timestamps.append([onset_time, offset_time])\n",
    "            elif note_event[\"pitch\"] > next_note_event[\"pitch\"]:\n",
    "                ref_pull_timestamps.append([onset_time, offset_time])\n",
    "        if note_event[\"effects\"][\"slide\"]:\n",
    "            ref_slide_timestamps.append([onset_time, offset_time])\n",
    "\n",
    "    ref_bend_timestamps = np.array(ref_bend_timestamps)\n",
    "    ref_vibrato_timestamps = np.array(ref_vibrato_timestamps)\n",
    "    ref_hammer_timestamps = np.array(ref_hammer_timestamps)\n",
    "    ref_pull_timestamps = np.array(ref_pull_timestamps)\n",
    "    ref_slide_timestamps = np.array(ref_slide_timestamps)\n",
    "\n",
    "    # matching and counting scores\n",
    "    bend_anno_cnt = len(ref_bend_timestamps)\n",
    "    bend_est_cnt = len(bend_timestamps)\n",
    "    if bend_anno_cnt == 0 or bend_est_cnt == 0:\n",
    "        bend_matching_cnt = 0\n",
    "    else:\n",
    "        bend_onset_matching = mir_eval.transcription.match_note_onsets(ref_bend_timestamps, bend_timestamps)\n",
    "        bend_offset_matching = mir_eval.transcription.match_note_offsets(ref_bend_timestamps, bend_timestamps)\n",
    "        bend_matching = set(bend_onset_matching+bend_offset_matching)\n",
    "        bend_matching_cnt = len(bend_matching)\n",
    "\n",
    "    vibrato_anno_cnt = len(ref_vibrato_timestamps)\n",
    "    vibrato_est_cnt = len(vibrato_timestamps)\n",
    "    if vibrato_anno_cnt == 0 or vibrato_est_cnt == 0:\n",
    "        vibrato_matching_cnt = 0\n",
    "    else:\n",
    "        vibrato_onset_matching = mir_eval.transcription.match_note_onsets(ref_vibrato_timestamps, vibrato_timestamps)\n",
    "        vibrato_offset_matching = mir_eval.transcription.match_note_offsets(ref_vibrato_timestamps, vibrato_timestamps)\n",
    "        vibrato_matching = set(vibrato_onset_matching+vibrato_offset_matching)\n",
    "        vibrato_matching_cnt = len(vibrato_matching)\n",
    "\n",
    "    hammer_anno_cnt = len(ref_hammer_timestamps)\n",
    "    hammer_est_cnt = len(hammer_timestamps)\n",
    "    if hammer_anno_cnt == 0 or hammer_est_cnt == 0:\n",
    "        hammer_matching_cnt = 0\n",
    "    else:\n",
    "        hammer_matching = mir_eval.transcription.match_note_offsets(ref_hammer_timestamps, hammer_timestamps)\n",
    "        hammer_matching_cnt = len(hammer_matching)\n",
    "\n",
    "    pull_anno_cnt = len(ref_pull_timestamps)\n",
    "    pull_est_cnt = len(pull_timestamps)\n",
    "    if pull_anno_cnt == 0 or pull_est_cnt == 0:\n",
    "        pull_matching_cnt = 0\n",
    "    else:\n",
    "        pull_matching = mir_eval.transcription.match_note_offsets(ref_pull_timestamps, pull_timestamps)\n",
    "        pull_matching_cnt = len(pull_matching)\n",
    "\n",
    "    slide_anno_cnt = len(ref_slide_timestamps)\n",
    "    slide_est_cnt = len(slide_timestamps)\n",
    "    if slide_anno_cnt == 0 or slide_est_cnt == 0:\n",
    "        slide_matching_cnt = 0\n",
    "    else:\n",
    "        slide_matching = mir_eval.transcription.match_note_offsets(ref_slide_timestamps, slide_timestamps)\n",
    "        slide_matching_cnt = len(slide_matching)\n",
    "\n",
    "    result_matrix = np.array(\n",
    "        [\n",
    "            [bend_anno_cnt, bend_est_cnt, bend_matching_cnt], \n",
    "            [vibrato_anno_cnt, vibrato_est_cnt, vibrato_matching_cnt],\n",
    "            [hammer_anno_cnt, hammer_est_cnt, hammer_matching_cnt],\n",
    "            [pull_anno_cnt, pull_est_cnt, pull_matching_cnt],\n",
    "            [slide_anno_cnt, slide_est_cnt, slide_matching_cnt]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return result_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this result is using backtrack=True in onset detection\n",
    "total_result_matrix = np.zeros((5, 3))\n",
    "for result_file in glob.glob(os.path.join(RESULT_NOTE_DIR, \"*.csv\")):\n",
    "    result_file = result_file.split(\"/\")[-1]\n",
    "    try:\n",
    "        result_matrix = evaluate_e2e(result_file)\n",
    "    except IndexError:\n",
    "        continue\n",
    "    total_result_matrix += result_matrix\n",
    "print(total_result_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = total_result_matrix[:, 2]\n",
    "anno = total_result_matrix[:, 0]\n",
    "est = total_result_matrix[:, 1]\n",
    "precision = match / est\n",
    "recall = match / anno\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e4299a6bfeee03721df81a04beb561ffa0442fcfce618d29889931a6d6e4527"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
